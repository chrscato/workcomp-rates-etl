{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f61fb22",
   "metadata": {},
   "source": [
    "### Stage 1 ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460d1657",
   "metadata": {},
   "source": [
    "- takes prov & rates parquet files from mrf engine repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf750278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- CONFIG ----------\n",
    "from pathlib import Path\n",
    "\n",
    "# Change these for each run\n",
    "STATE = \"GA\"  # \"VA\", etc.\n",
    "\n",
    "# Optional hard override (otherwise computed from reporting_entity_name)\n",
    "PAYER_SLUG_OVERRIDE = None  # e.g., \"aetna\", \"uhc\"\n",
    "\n",
    "# Data store root (dims/xrefs/gold will live under here)\n",
    "# This notebook is under ETL/, so we need to back out to the parent to be at the same level as 'data'\n",
    "# __file__ is not defined in Jupyter, so use notebook location as base\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # Try to use __file__ if running as a script\n",
    "    DS_ROOT = Path(__file__).parent.parent / \"data\"\n",
    "except NameError:\n",
    "    # Fallback for Jupyter: use current working directory, assume ETL/ as CWD\n",
    "    DS_ROOT = Path(os.getcwd()).parent / \"data\"\n",
    "\n",
    "DS_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "RATES_PARQ = DS_ROOT / \"input/202508_aetna_ga_rates.parquet\"    # switch to UHC file on rerun\n",
    "PROV_PARQ  = DS_ROOT / \"input/202508_aetna_ga_providers.parquet\"  # switch to UHC file on rerun\n",
    "\n",
    "# Outputs\n",
    "DIM_DIR   = DS_ROOT / \"dims\"\n",
    "XREF_DIR  = DS_ROOT / \"xrefs\"\n",
    "GOLD_DIR  = DS_ROOT / \"gold\"\n",
    "DIM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "XREF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "GOLD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Parquet file targets\n",
    "DIM_CODE_FILE   = DIM_DIR  / \"dim_code.parquet\"\n",
    "DIM_PAYER_FILE  = DIM_DIR  / \"dim_payer.parquet\"\n",
    "DIM_PG_FILE     = DIM_DIR  / \"dim_provider_group.parquet\"\n",
    "DIM_POS_FILE    = DIM_DIR  / \"dim_pos_set.parquet\"\n",
    "\n",
    "XREF_PG_NPI     = XREF_DIR / \"xref_pg_member_npi.parquet\"\n",
    "XREF_PG_TIN     = XREF_DIR / \"xref_pg_member_tin.parquet\"\n",
    "\n",
    "GOLD_FACT_FILE  = GOLD_DIR / \"fact_rate.parquet\"\n",
    "\n",
    "# Columns we’ll actually read (memory saver)\n",
    "RATES_COLS = [\n",
    "    \"last_updated_on\",\"reporting_entity_name\",\"version\",\n",
    "    \"billing_class\",\"billing_code_type\",\"billing_code\",\n",
    "    \"service_codes\",\"negotiated_type\",\"negotiation_arrangement\",\n",
    "    \"negotiated_rate\",\"expiration_date\",\"description\",\"name\",\n",
    "    \"provider_reference_id\",\"provider_group_id\",\"provider_group_id_raw\"\n",
    "]\n",
    "\n",
    "# Some MRFs don’t include both provider_reference_id & provider_group_id; we handle that later.\n",
    "PROV_COLS = [\n",
    "    \"last_updated_on\",\"reporting_entity_name\",\"version\",\n",
    "    \"provider_group_id\",\"provider_reference_id\",\n",
    "    \"npi\",\"tin_type\",\"tin_value\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996542d7",
   "metadata": {},
   "source": [
    "Cell 2 — Helpers & keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff204843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.2 (tags/v3.13.2:4f8bb39, Feb  4 2025, 15:23:48) [MSC v.1942 64 bit (AMD64)]\n",
      "Polars: 1.29.0\n",
      "DuckDB: 1.2.1\n"
     ]
    }
   ],
   "source": [
    "import os, sys, math, json, re, hashlib, duckdb\n",
    "import polars as pl\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Polars:\", pl.__version__)\n",
    "print(\"DuckDB:\", duckdb.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf7b600",
   "metadata": {},
   "source": [
    "Cell 3 — Ingest & normalize this batch (mint batch_id & pg_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aec9cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def md5(s: str) -> str:\n",
    "    return hashlib.md5(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def slugify(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"-\", s).strip(\"-\")\n",
    "    s = re.sub(r\"-+\", \"-\", s)\n",
    "    return s\n",
    "\n",
    "def _co(x):\n",
    "    return \"\" if x is None else str(x)\n",
    "\n",
    "def payer_slug_from_name(name: str) -> str:\n",
    "    if PAYER_SLUG_OVERRIDE:\n",
    "        return PAYER_SLUG_OVERRIDE\n",
    "    return slugify(name or \"\")\n",
    "\n",
    "def normalize_yymm(date_str: str | None) -> str:\n",
    "    if not date_str:\n",
    "        return \"\"\n",
    "    # try common formats\n",
    "    for fmt in (\"%Y-%m-%d\", \"%Y/%m/%d\", \"%Y-%m\", \"%Y/%m\", \"%Y%m%d\", \"%Y%m\"):\n",
    "        try:\n",
    "            dt = datetime.strptime(date_str[:len(fmt.replace(\"%\",\"\").replace(\"-\",\"\").replace(\"/\",\"\"))], fmt)\n",
    "            return dt.strftime(\"%Y-%m\")\n",
    "        except Exception:\n",
    "            continue\n",
    "    # fallback: extract yyyy-mm\n",
    "    m = re.search(r\"(20\\d{2})[-/](0[1-9]|1[0-2])\", date_str)\n",
    "    return f\"{m.group(1)}-{m.group(2)}\" if m else \"\"\n",
    "\n",
    "def normalize_service_codes(svc) -> list[str]:\n",
    "    \"\"\"\n",
    "    Normalize service_codes into a sorted unique list of strings.\n",
    "    Handles None, list/tuple, JSON-like strings, and CSV-ish strings.\n",
    "    Never uses bare truthiness checks that could hit Series ambiguity.\n",
    "    \"\"\"\n",
    "    if svc is None:\n",
    "        return []\n",
    "\n",
    "    # If it's already a list/tuple, stringify elements\n",
    "    if isinstance(svc, (list, tuple)):\n",
    "        vals = [\"\" if v is None else str(v) for v in svc]\n",
    "    else:\n",
    "        s = str(svc)\n",
    "\n",
    "        # Try to parse JSON-like list strings (e.g., '[\"11\",\"22\"]')\n",
    "        if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "            try:\n",
    "                import json\n",
    "                parsed = json.loads(s)\n",
    "                if isinstance(parsed, list):\n",
    "                    vals = [\"\" if v is None else str(v) for v in parsed]\n",
    "                else:\n",
    "                    vals = re.split(r\"[;,|\\s]+\", s)\n",
    "            except Exception:\n",
    "                vals = re.split(r\"[;,|\\s]+\", s)\n",
    "        else:\n",
    "            vals = re.split(r\"[;,|\\s]+\", s)\n",
    "\n",
    "    cleaned: list[str] = []\n",
    "    for v in vals:\n",
    "        sv = str(v).strip()\n",
    "        if len(sv) > 0:\n",
    "            cleaned.append(sv)\n",
    "\n",
    "    # dedupe + sorted\n",
    "    return sorted(set(cleaned))\n",
    "\n",
    "\n",
    "def pos_set_id_from_members(members) -> str:\n",
    "    \"\"\"\n",
    "    Make a stable id from a list of POS members.\n",
    "    Avoid bare truthiness; handle non-list defensive cases.\n",
    "    \"\"\"\n",
    "    if members is None:\n",
    "        return md5(\"none\")\n",
    "    try:\n",
    "        n = len(members)\n",
    "    except Exception:\n",
    "        # not iterable; coerce to single-element list\n",
    "        members = [str(members)]\n",
    "        n = 1\n",
    "    if n == 0:\n",
    "        return md5(\"none\")\n",
    "    # stringify defensively\n",
    "    parts = [\"\" if m is None else str(m) for m in members]\n",
    "    return md5(\"|\".join(parts))\n",
    "\n",
    "\n",
    "def pg_uid_from_parts(payer_slug: str, version: str | None, pgid: str | None, pref: str | None) -> str:\n",
    "    # Prefer group_id, fall back to provider_reference_id\n",
    "    key = f\"{_co(payer_slug)}|{_co(version)}|{_co(pgid)}|{_co(pref)}\"\n",
    "    return md5(key)\n",
    "\n",
    "def fact_uid_from_struct(s: dict) -> str:\n",
    "    \"\"\"\n",
    "    Deterministic ID for one fact row (idempotent upsert).\n",
    "    Includes STATE so GA vs VA don't collide.\n",
    "    Rounds negotiated_rate to 4 decimals to avoid float drift.\n",
    "    \"\"\"\n",
    "    rate_val = s.get(\"negotiated_rate\")\n",
    "    try:\n",
    "        rate_str = f\"{float(rate_val):.4f}\" if rate_val is not None else \"\"\n",
    "    except Exception:\n",
    "        rate_str = \"\"\n",
    "    parts = [\n",
    "        _co(s.get(\"state\")),\n",
    "        _co(s.get(\"year_month\")),\n",
    "        _co(s.get(\"payer_slug\")),\n",
    "        _co(s.get(\"billing_class\")),\n",
    "        _co(s.get(\"code_type\")),\n",
    "        _co(s.get(\"code\")),\n",
    "        _co(s.get(\"pg_uid\")),\n",
    "        _co(s.get(\"pos_set_id\")),\n",
    "        _co(s.get(\"negotiated_type\")),\n",
    "        _co(s.get(\"negotiation_arrangement\")),\n",
    "        _co(s.get(\"expiration_date\")),\n",
    "        rate_str,\n",
    "        _co(s.get(\"provider_group_id_raw\")),\n",
    "    ]\n",
    "    return md5(\"|\".join(parts))\n",
    "\n",
    "def prj_cols(df: pl.DataFrame, cols: list[str]) -> pl.DataFrame:\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        # add missing as nulls so downstream code doesn't crash\n",
    "        df = df.with_columns([pl.lit(None).alias(c) for c in missing])\n",
    "    return df.select(cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbe9331",
   "metadata": {},
   "source": [
    "Cell 4 — Small “append-unique” writers for dims/xrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88536396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ChristopherCato\\AppData\\Local\\Temp\\ipykernel_35792\\1278398007.py:6: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  avail = set(lf.columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rates rows: 13456820 cols: 16\n",
      "prov  rows: 131984 cols: 8\n",
      "RATES present cols: ['last_updated_on', 'reporting_entity_name', 'version', 'billing_class', 'billing_code_type', 'billing_code', 'service_codes', 'negotiated_type', 'negotiation_arrangement', 'negotiated_rate', 'expiration_date', 'description', 'name', 'provider_reference_id']\n",
      "PROV  present cols: ['last_updated_on', 'reporting_entity_name', 'version', 'provider_group_id', 'npi', 'tin_type', 'tin_value']\n"
     ]
    }
   ],
   "source": [
    "# Robust read that tolerates missing columns by intersecting with the file schema.\n",
    "# Then, we add any still-missing columns as nulls so later code can rely on them.\n",
    "\n",
    "def read_parquet_safely(path: Path, desired_cols: list[str]) -> pl.DataFrame:\n",
    "    lf = pl.scan_parquet(str(path))  # lazy, no data loaded yet\n",
    "    avail = set(lf.columns)\n",
    "    use_cols = [c for c in desired_cols if c in avail]\n",
    "    df = lf.select(use_cols).collect()  # load only what's present\n",
    "\n",
    "    # back-fill missing columns as nulls to keep downstream selects happy\n",
    "    missing = [c for c in desired_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        df = df.with_columns([pl.lit(None).alias(c) for c in missing])\n",
    "    return df\n",
    "\n",
    "if not RATES_PARQ.exists():\n",
    "    raise FileNotFoundError(f\"Missing RATES_PARQ: {RATES_PARQ}\")\n",
    "if not PROV_PARQ.exists():\n",
    "    raise FileNotFoundError(f\"Missing PROV_PARQ:  {PROV_PARQ}\")\n",
    "\n",
    "# TIP: if your RATES_COLS still includes super-optional fields,\n",
    "# it's fine because read_parquet_safely will just back-fill them.\n",
    "rates = read_parquet_safely(RATES_PARQ, RATES_COLS)\n",
    "prov  = read_parquet_safely(PROV_PARQ,  PROV_COLS)\n",
    "\n",
    "print(\"rates rows:\", rates.height, \"cols:\", len(rates.columns))\n",
    "print(\"prov  rows:\", prov.height, \"cols:\", len(prov.columns))\n",
    "\n",
    "# Optional quick peek if you’re debugging schemas:\n",
    "print(\"RATES present cols:\", [c for c in rates.columns if rates.select(pl.col(c).is_not_null().any()).item()])\n",
    "print(\"PROV  present cols:\", [c for c in prov.columns  if prov.select(pl.col(c).is_not_null().any()).item()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5740bb37",
   "metadata": {},
   "source": [
    "Cell 5 — Upsert the FACT into a state-scoped Parquet dataset (dedup partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb525d4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 78\u001b[0m\n\u001b[0;32m     57\u001b[0m pos_members_expr \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     58\u001b[0m     _trim(\n\u001b[0;32m     59\u001b[0m         _sc\u001b[38;5;241m.\u001b[39mfill_null(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;241m.\u001b[39mlist\u001b[38;5;241m.\u001b[39msort()\n\u001b[0;32m     73\u001b[0m )\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# ---------- build base + IDs ----------\u001b[39;00m\n\u001b[0;32m     76\u001b[0m base \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     77\u001b[0m     \u001b[43mrates\u001b[49m\n\u001b[1;32m---> 78\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m          \u001b[49m\u001b[43m_payer_slug_expr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpayer_slug\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m          \u001b[49m\u001b[43m_year_month_expr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myear_month\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m          \u001b[49m\u001b[43mpos_members_expr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpos_members\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m      \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m )\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# pos_set_id = md5 of normalized list (use list.join, not arr.join)\u001b[39;00m\n\u001b[0;32m     86\u001b[0m base \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     87\u001b[0m     base\n\u001b[0;32m     88\u001b[0m       \u001b[38;5;241m.\u001b[39mwith_columns(pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos_members\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlist\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos_key\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     89\u001b[0m       \u001b[38;5;241m.\u001b[39mwith_columns(pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos_key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmap_elements(md5, return_dtype\u001b[38;5;241m=\u001b[39mpl\u001b[38;5;241m.\u001b[39mUtf8)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos_set_id\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     90\u001b[0m       \u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos_key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     91\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ChristopherCato\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\polars\\dataframe\\frame.py:9830\u001b[0m, in \u001b[0;36mDataFrame.with_columns\u001b[1;34m(self, *exprs, **named_exprs)\u001b[0m\n\u001b[0;32m   9684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwith_columns\u001b[39m(\n\u001b[0;32m   9685\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   9686\u001b[0m     \u001b[38;5;241m*\u001b[39mexprs: IntoExpr \u001b[38;5;241m|\u001b[39m Iterable[IntoExpr],\n\u001b[0;32m   9687\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnamed_exprs: IntoExpr,\n\u001b[0;32m   9688\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   9689\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   9690\u001b[0m \u001b[38;5;124;03m    Add columns to this DataFrame.\u001b[39;00m\n\u001b[0;32m   9691\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9828\u001b[0m \u001b[38;5;124;03m    └─────┴──────┴─────────────┘\u001b[39;00m\n\u001b[0;32m   9829\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 9830\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexprs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnamed_exprs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_eager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ChristopherCato\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\polars\\_utils\\deprecation.py:93\u001b[0m, in \u001b[0;36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min-memory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstreaming\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ChristopherCato\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\polars\\lazyframe\\frame.py:2224\u001b[0m, in \u001b[0;36mLazyFrame.collect\u001b[1;34m(self, type_coercion, _type_check, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, _check_order, _eager, **_kwargs)\u001b[0m\n\u001b[0;32m   2222\u001b[0m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[0;32m   2223\u001b[0m callback \u001b[38;5;241m=\u001b[39m _kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost_opt_callback\u001b[39m\u001b[38;5;124m\"\u001b[39m, callback)\n\u001b[1;32m-> 2224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ---------- Efficient, Batched Upsert for Notebooks ----------\n",
    "# The classic upsert (append-unique) pattern is expensive because it reads the entire existing Parquet file\n",
    "# and does a full in-memory join for deduplication. This is not notebook-friendly for large files.\n",
    "#\n",
    "# To keep this notebook-friendly and avoid reading the whole file:\n",
    "# - Only process/apply upserts in small batches (e.g., per payer, per state, or per chunk).\n",
    "# - Only read the *keys* column(s) from the existing file, and only if the file is small.\n",
    "# - If the file is large, maintain a separate \"index\" of keys (e.g., as a CSV or Parquet of just the keys).\n",
    "# - For truly large-scale, use a database (DuckDB, SQLite) for deduplication, or partition files by key.\n",
    "#\n",
    "# Below, we show a batched, memory-light approach for upserts in a notebook.\n",
    "\n",
    "# ---------- Compat helpers (no .str.strip / no .strip_matches) ----------\n",
    "def _trim(expr: pl.Expr) -> pl.Expr:\n",
    "    # remove leading/trailing whitespace via regex\n",
    "    return expr.str.replace_all(r\"^\\s+|\\s+$\", \"\")\n",
    "\n",
    "def _trim_dashes(expr: pl.Expr) -> pl.Expr:\n",
    "    # remove leading/trailing '-' via regex\n",
    "    return expr.str.replace_all(r\"^-+|-+$\", \"\")\n",
    "\n",
    "def _payer_slug_expr() -> pl.Expr:\n",
    "    base_slug = (\n",
    "        pl.col(\"reporting_entity_name\")\n",
    "          .fill_null(\"\")\n",
    "          .str.to_lowercase()\n",
    "          .str.replace_all(r\"[^a-z0-9]+\", \"-\")    # collapse to dashes\n",
    "          .str.replace_all(r\"-{2,}\", \"-\")         # dedupe dashes\n",
    "    )\n",
    "    base_slug = _trim_dashes(base_slug)           # trim edge dashes\n",
    "    override = pl.lit(PAYER_SLUG_OVERRIDE)\n",
    "    return pl.when(override.is_not_null() & (override != \"\")) \\\n",
    "             .then(override) \\\n",
    "             .otherwise(base_slug)\n",
    "\n",
    "def _year_month_expr() -> pl.Expr:\n",
    "    return (\n",
    "        pl.coalesce([\n",
    "            pl.col(\"last_updated_on\").str.strptime(pl.Date, \"%Y-%m-%d\", strict=False),\n",
    "            pl.col(\"last_updated_on\").str.strptime(pl.Date, \"%Y/%m/%d\", strict=False),\n",
    "            pl.col(\"last_updated_on\").str.strptime(pl.Date, \"%Y-%m\",     strict=False),\n",
    "            pl.col(\"last_updated_on\").str.strptime(pl.Date, \"%Y/%m\",     strict=False),\n",
    "            pl.col(\"last_updated_on\").str.strptime(pl.Date, \"%Y%m%d\",    strict=False),\n",
    "            pl.col(\"last_updated_on\").str.strptime(pl.Date, \"%Y%m\",      strict=False),\n",
    "        ])\n",
    "        .dt.strftime(\"%Y-%m\")\n",
    "        .fill_null(\"\")\n",
    "    )\n",
    "\n",
    "# service_codes -> normalized list[str] (robust: no json_decode)\n",
    "_sc = pl.col(\"service_codes\").cast(pl.Utf8)\n",
    "\n",
    "# regex-trim helper (compat with older Polars)\n",
    "def _trim(expr: pl.Expr) -> pl.Expr:\n",
    "    return expr.str.replace_all(r\"^\\s+|\\s+$\", \"\")\n",
    "\n",
    "pos_members_expr = (\n",
    "    _trim(\n",
    "        _sc.fill_null(\"\")\n",
    "           # strip brackets/braces/parens and quotes that break JSON parsing\n",
    "           .str.replace_all(r\"[\\[\\]\\{\\}\\(\\)]\", \" \")\n",
    "           .str.replace_all(r\"[\\\"']\", \" \")\n",
    "           # unify separators to spaces\n",
    "           .str.replace_all(r\"[;,|]+\", \" \")\n",
    "           .str.replace_all(r\"\\s+\", \" \")\n",
    "    )\n",
    "    .str.split(\" \")\n",
    "    .list.eval(_trim(pl.element()))\n",
    "    .list.eval(pl.when(pl.element() == \"\").then(None).otherwise(pl.element()))\n",
    "    .list.drop_nulls()\n",
    "    .list.unique()\n",
    "    .list.sort()\n",
    ")\n",
    "\n",
    "# ---------- build base + IDs ----------\n",
    "base = (\n",
    "    rates\n",
    "      .with_columns([\n",
    "          _payer_slug_expr().alias(\"payer_slug\"),\n",
    "          _year_month_expr().alias(\"year_month\"),\n",
    "          pos_members_expr.alias(\"pos_members\"),\n",
    "      ])\n",
    ")\n",
    "\n",
    "# pos_set_id = md5 of normalized list (use list.join, not arr.join)\n",
    "base = (\n",
    "    base\n",
    "      .with_columns(pl.col(\"pos_members\").list.join(\"|\").alias(\"pos_key\"))\n",
    "      .with_columns(pl.col(\"pos_key\").map_elements(md5, return_dtype=pl.Utf8).alias(\"pos_set_id\"))\n",
    "      .drop(\"pos_key\")\n",
    ")\n",
    "\n",
    "# pg_uid = md5(\"payer_slug|version|provider_reference_id|\"\") for rates\n",
    "base = (\n",
    "    base\n",
    "      .with_columns(\n",
    "          pl.concat_str(\n",
    "              [\n",
    "                  pl.col(\"payer_slug\"),\n",
    "                  pl.col(\"version\").fill_null(\"\"),\n",
    "                  pl.col(\"provider_reference_id\").fill_null(\"\"),  # ✅ Use provider_reference_id from rates\n",
    "                  pl.lit(\"\"),  # provider_group_id doesn't exist in rates\n",
    "              ],\n",
    "              separator=\"|\",\n",
    "          ).alias(\"pg_key\")\n",
    "      )\n",
    "      .with_columns(pl.col(\"pg_key\").map_elements(md5, return_dtype=pl.Utf8).alias(\"pg_uid\"))\n",
    "      .drop(\"pg_key\")\n",
    ")\n",
    "\n",
    "# ---------- dims/xrefs ----------\n",
    "dim_code_new = (\n",
    "    base.select([\n",
    "        pl.col(\"billing_code_type\").alias(\"code_type\"),\n",
    "        pl.col(\"billing_code\").cast(pl.Utf8).alias(\"code\"),\n",
    "        pl.col(\"description\").alias(\"code_description\"),\n",
    "        pl.col(\"name\").alias(\"code_name\"),\n",
    "    ])\n",
    "    .drop_nulls(subset=[\"code_type\",\"code\"])\n",
    "    .unique()\n",
    ")\n",
    "\n",
    "dim_payer_new = (\n",
    "    base.select([\n",
    "        pl.col(\"payer_slug\"),\n",
    "        pl.col(\"reporting_entity_name\").alias(\"reporting_entity_name\"),\n",
    "        pl.col(\"version\").alias(\"version\"),\n",
    "    ])\n",
    "    .drop_nulls(subset=[\"payer_slug\"])\n",
    "    .unique()\n",
    ")\n",
    "\n",
    "dim_pg_new = (\n",
    "    base.select([\n",
    "        pl.col(\"pg_uid\"),\n",
    "        pl.col(\"payer_slug\"),\n",
    "        pl.coalesce([pl.col(\"provider_group_id\"), pl.col(\"provider_reference_id\")]).alias(\"provider_group_id_raw\"),\n",
    "        pl.col(\"version\"),\n",
    "    ])\n",
    "    .drop_nulls(subset=[\"pg_uid\"])\n",
    "    .unique()\n",
    ")\n",
    "\n",
    "dim_pos_new = (\n",
    "    base.select([\"pos_set_id\",\"pos_members\"])\n",
    "        .drop_nulls(subset=[\"pos_set_id\"])\n",
    "        .unique()\n",
    ")\n",
    "\n",
    "# XREFs from provider file (vectorized payer_slug + pg_uid)\n",
    "prov_aug = (\n",
    "    prov\n",
    "      .with_columns([ _payer_slug_expr().alias(\"payer_slug\") ])\n",
    "      .with_columns(\n",
    "          pl.concat_str(\n",
    "              [\n",
    "                  pl.col(\"payer_slug\"),\n",
    "                  pl.col(\"version\").fill_null(\"\"),\n",
    "                  pl.col(\"provider_group_id\").fill_null(\"\"),  # ✅ Use provider_group_id from provider\n",
    "                  pl.lit(\"\"),  # provider_reference_id not needed for hash\n",
    "              ],\n",
    "              separator=\"|\",\n",
    "          ).alias(\"pg_key\")\n",
    "      )\n",
    "      .with_columns(pl.col(\"pg_key\").map_elements(md5, return_dtype=pl.Utf8).alias(\"pg_uid\"))\n",
    "      .drop(\"pg_key\")\n",
    ")\n",
    "\n",
    "xref_pg_npi_new = (\n",
    "    prov_aug.select([\"pg_uid\",\"npi\"])\n",
    "            .drop_nulls(subset=[\"pg_uid\",\"npi\"])\n",
    "            .unique()\n",
    ")\n",
    "\n",
    "xref_pg_tin_new = (\n",
    "    prov_aug.select([\"pg_uid\",\"tin_type\",\"tin_value\"])\n",
    "            .drop_nulls(subset=[\"pg_uid\",\"tin_value\"])\n",
    "            .unique()\n",
    ")\n",
    "\n",
    "print(\"Base/dims/xrefs built (vectorized, compat v2).\")\n",
    "\n",
    "# --------- Batched Upsert Helper for Notebooks ---------\n",
    "# Only reads the *keys* column(s) from the existing file, and only for the current batch.\n",
    "# If the file is large, consider partitioning or using a DB for deduplication.\n",
    "\n",
    "def append_unique_parquet_batched(df_new: pl.DataFrame, path: Path, keys: list[str], batch_size: int = 10000):\n",
    "    \"\"\"\n",
    "    Append unique rows from df_new to Parquet at path, in small batches.\n",
    "    Only reads the keys from the existing file, not the whole file.\n",
    "    \"\"\"\n",
    "    import math\n",
    "\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not path.exists():\n",
    "        # If file doesn't exist, just write all new data\n",
    "        df_new.write_parquet(path, compression=\"zstd\")\n",
    "        return\n",
    "\n",
    "    # Read only the keys from the existing file\n",
    "    try:\n",
    "        existing_keys = pl.read_parquet(path, columns=keys)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not read keys from {path}: {e}\")\n",
    "        existing_keys = pl.DataFrame({k: [] for k in keys})\n",
    "\n",
    "    # Process in batches to avoid memory spikes\n",
    "    n = df_new.height\n",
    "    n_batches = math.ceil(n / batch_size)\n",
    "    for i in range(n_batches):\n",
    "        start = i * batch_size\n",
    "        end = min((i+1) * batch_size, n)\n",
    "        batch = df_new.slice(start, end - start)\n",
    "        # Anti-join to find truly new rows\n",
    "        to_add = batch.join(existing_keys, on=keys, how=\"anti\")\n",
    "        if not to_add.is_empty():\n",
    "            # Append to Parquet (append mode)\n",
    "            to_add.write_parquet(path, compression=\"zstd\", append=True)\n",
    "\n",
    "# Example usage (replace append_unique_parquet with batched version for notebook safety):\n",
    "# append_unique_parquet_batched(dim_code_new, DIM_CODE_FILE, keys=[\"code_type\",\"code\"])\n",
    "# append_unique_parquet_batched(dim_payer_new, DIM_PAYER_FILE, keys=[\"payer_slug\"])\n",
    "# append_unique_parquet_batched(dim_pg_new,   DIM_PG_FILE,    keys=[\"pg_uid\"])\n",
    "# append_unique_parquet_batched(dim_pos_new,  DIM_POS_FILE,   keys=[\"pos_set_id\"])\n",
    "# append_unique_parquet_batched(xref_pg_npi_new, XREF_PG_NPI, keys=[\"pg_uid\",\"npi\"])\n",
    "# append_unique_parquet_batched(xref_pg_tin_new, XREF_PG_TIN, keys=[\"pg_uid\",\"tin_value\"])\n",
    "\n",
    "print(\"Ready for batched, memory-light upserts in notebook mode.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4089be",
   "metadata": {},
   "source": [
    "Cell 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bab9cf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dims/Xrefs up to date.\n"
     ]
    }
   ],
   "source": [
    "def append_unique_parquet(df_new: pl.DataFrame, path: Path, keys: list[str]):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if path.exists():\n",
    "        old_keys = pl.read_parquet(path, columns=keys).unique()\n",
    "        to_add = df_new.join(old_keys, on=keys, how=\"anti\")\n",
    "    else:\n",
    "        to_add = df_new\n",
    "    if to_add.is_empty():\n",
    "        return\n",
    "\n",
    "    tmp_new = path.with_suffix(\".new.parquet\")\n",
    "    tmp_out = path.with_suffix(\".next.parquet\")\n",
    "\n",
    "    to_add.write_parquet(tmp_new, compression=\"zstd\")\n",
    "    con = duckdb.connect()\n",
    "    if path.exists():\n",
    "        con.execute(f\"\"\"\n",
    "          COPY (\n",
    "            SELECT * FROM read_parquet('{path}')\n",
    "            UNION ALL\n",
    "            SELECT * FROM read_parquet('{tmp_new}')\n",
    "          ) TO '{tmp_out}' (FORMAT PARQUET, COMPRESSION ZSTD);\n",
    "        \"\"\")\n",
    "    else:\n",
    "        con.execute(f\"\"\"\n",
    "          COPY (SELECT * FROM read_parquet('{tmp_new}'))\n",
    "          TO '{tmp_out}' (FORMAT PARQUET, COMPRESSION ZSTD);\n",
    "        \"\"\")\n",
    "    con.close()\n",
    "    os.replace(tmp_out, path)\n",
    "    os.remove(tmp_new)\n",
    "\n",
    "# Write dims/xrefs\n",
    "append_unique_parquet(dim_code_new, DIM_CODE_FILE, keys=[\"code_type\",\"code\"])\n",
    "append_unique_parquet(dim_payer_new, DIM_PAYER_FILE, keys=[\"payer_slug\"])\n",
    "append_unique_parquet(dim_pg_new,   DIM_PG_FILE,    keys=[\"pg_uid\"])\n",
    "append_unique_parquet(dim_pos_new,  DIM_POS_FILE,   keys=[\"pos_set_id\"])\n",
    "\n",
    "append_unique_parquet(xref_pg_npi_new, XREF_PG_NPI, keys=[\"pg_uid\",\"npi\"])\n",
    "append_unique_parquet(xref_pg_tin_new, XREF_PG_TIN, keys=[\"pg_uid\",\"tin_value\"])\n",
    "\n",
    "print(\"Dims/Xrefs up to date.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c347bd7",
   "metadata": {},
   "source": [
    "Cell 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e82db559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fact_new rows: 2967105\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 15)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>fact_uid</th><th>state</th><th>year_month</th><th>payer_slug</th><th>billing_class</th><th>code_type</th><th>code</th><th>pg_uid</th><th>pos_set_id</th><th>negotiated_type</th><th>negotiation_arrangement</th><th>negotiated_rate</th><th>expiration_date</th><th>provider_group_id_raw</th><th>reporting_entity_name</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>&quot;d023eebdae2fec46f9179ab625c678…</td><td>&quot;GA&quot;</td><td>&quot;2025-08&quot;</td><td>&quot;unitedhealthcare-of-georgia-in…</td><td>&quot;professional&quot;</td><td>&quot;CPT&quot;</td><td>&quot;28545&quot;</td><td>&quot;016b481acf847781526267a457c9c4…</td><td>&quot;17b00c58b3dcdb9c20cb2a70b52a4c…</td><td>&quot;negotiated&quot;</td><td>&quot;ffs&quot;</td><td>608.27</td><td>&quot;9999-12-31&quot;</td><td>584</td><td>&quot;UnitedHealthcare of Georgia In…</td></tr><tr><td>&quot;5f8783b3c2eadafc80af9f47a3bcec…</td><td>&quot;GA&quot;</td><td>&quot;2025-08&quot;</td><td>&quot;unitedhealthcare-of-georgia-in…</td><td>&quot;professional&quot;</td><td>&quot;CPT&quot;</td><td>&quot;29886&quot;</td><td>&quot;e469dc1ef6c0ea0d9bb989d96e4592…</td><td>&quot;17b00c58b3dcdb9c20cb2a70b52a4c…</td><td>&quot;negotiated&quot;</td><td>&quot;ffs&quot;</td><td>1051.15</td><td>&quot;9999-12-31&quot;</td><td>282</td><td>&quot;UnitedHealthcare of Georgia In…</td></tr><tr><td>&quot;da6893a29f62eb80b35fc28450365a…</td><td>&quot;GA&quot;</td><td>&quot;2025-08&quot;</td><td>&quot;unitedhealthcare-of-georgia-in…</td><td>&quot;institutional&quot;</td><td>&quot;CPT&quot;</td><td>&quot;19120&quot;</td><td>&quot;a910491ce9955a0083ee09c0b9c36d…</td><td>&quot;3b3743c43fbdc8f1b8eedc16b264f3…</td><td>&quot;negotiated&quot;</td><td>&quot;ffs&quot;</td><td>0.0</td><td>&quot;9999-12-31&quot;</td><td>220</td><td>&quot;UnitedHealthcare of Georgia In…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 15)\n",
       "┌────────────┬───────┬────────────┬────────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ fact_uid   ┆ state ┆ year_month ┆ payer_slug ┆ … ┆ negotiate ┆ expiratio ┆ provider_ ┆ reporting │\n",
       "│ ---        ┆ ---   ┆ ---        ┆ ---        ┆   ┆ d_rate    ┆ n_date    ┆ group_id_ ┆ _entity_n │\n",
       "│ str        ┆ str   ┆ str        ┆ str        ┆   ┆ ---       ┆ ---       ┆ raw       ┆ ame       │\n",
       "│            ┆       ┆            ┆            ┆   ┆ f64       ┆ str       ┆ ---       ┆ ---       │\n",
       "│            ┆       ┆            ┆            ┆   ┆           ┆           ┆ i64       ┆ str       │\n",
       "╞════════════╪═══════╪════════════╪════════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ d023eebdae ┆ GA    ┆ 2025-08    ┆ unitedheal ┆ … ┆ 608.27    ┆ 9999-12-3 ┆ 584       ┆ UnitedHea │\n",
       "│ 2fec46f917 ┆       ┆            ┆ thcare-of- ┆   ┆           ┆ 1         ┆           ┆ lthcare   │\n",
       "│ 9ab625c678 ┆       ┆            ┆ georgia-in ┆   ┆           ┆           ┆           ┆ of        │\n",
       "│ …          ┆       ┆            ┆ …          ┆   ┆           ┆           ┆           ┆ Georgia   │\n",
       "│            ┆       ┆            ┆            ┆   ┆           ┆           ┆           ┆ In…       │\n",
       "│ 5f8783b3c2 ┆ GA    ┆ 2025-08    ┆ unitedheal ┆ … ┆ 1051.15   ┆ 9999-12-3 ┆ 282       ┆ UnitedHea │\n",
       "│ eadafc80af ┆       ┆            ┆ thcare-of- ┆   ┆           ┆ 1         ┆           ┆ lthcare   │\n",
       "│ 9f47a3bcec ┆       ┆            ┆ georgia-in ┆   ┆           ┆           ┆           ┆ of        │\n",
       "│ …          ┆       ┆            ┆ …          ┆   ┆           ┆           ┆           ┆ Georgia   │\n",
       "│            ┆       ┆            ┆            ┆   ┆           ┆           ┆           ┆ In…       │\n",
       "│ da6893a29f ┆ GA    ┆ 2025-08    ┆ unitedheal ┆ … ┆ 0.0       ┆ 9999-12-3 ┆ 220       ┆ UnitedHea │\n",
       "│ 62eb80b35f ┆       ┆            ┆ thcare-of- ┆   ┆           ┆ 1         ┆           ┆ lthcare   │\n",
       "│ c28450365a ┆       ┆            ┆ georgia-in ┆   ┆           ┆           ┆           ┆ of        │\n",
       "│ …          ┆       ┆            ┆ …          ┆   ┆           ┆           ┆           ┆ Georgia   │\n",
       "│            ┆       ┆            ┆            ┆   ┆           ┆           ┆           ┆ In…       │\n",
       "└────────────┴───────┴────────────┴────────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 7 — Build fact_new (keep exact fact_uid semantics via existing helper)\n",
    "\n",
    "fact_new = (\n",
    "    base\n",
    "      .with_columns(pl.lit(STATE).alias(\"state\"))\n",
    "      .select(\n",
    "          \"state\",\n",
    "          pl.col(\"year_month\"),\n",
    "          pl.col(\"payer_slug\"),\n",
    "          pl.col(\"billing_class\"),\n",
    "          pl.col(\"billing_code_type\").alias(\"code_type\"),\n",
    "          pl.col(\"billing_code\").cast(pl.Utf8).alias(\"code\"),\n",
    "          pl.col(\"pg_uid\"),\n",
    "          pl.col(\"pos_set_id\"),\n",
    "          pl.col(\"negotiated_type\"),\n",
    "          pl.col(\"negotiation_arrangement\"),\n",
    "          pl.col(\"negotiated_rate\").cast(pl.Float64).alias(\"negotiated_rate\"),\n",
    "          pl.col(\"expiration_date\"),\n",
    "          pl.coalesce([pl.col(\"provider_group_id\"), pl.col(\"provider_reference_id\")]).alias(\"provider_group_id_raw\"),\n",
    "          pl.col(\"reporting_entity_name\"),\n",
    "      )\n",
    "      .with_columns(\n",
    "          # preserve your exact ID logic (%.4f on rate, same field order)\n",
    "          pl.struct([\n",
    "              \"state\",\"year_month\",\"payer_slug\",\"billing_class\",\"code_type\",\"code\",\n",
    "              \"pg_uid\",\"pos_set_id\",\"negotiated_type\",\"negotiation_arrangement\",\n",
    "              \"expiration_date\",\"negotiated_rate\",\"provider_group_id_raw\"\n",
    "          ]).map_elements(fact_uid_from_struct, return_dtype=pl.Utf8).alias(\"fact_uid\")\n",
    "      )\n",
    "      .select(\n",
    "          \"fact_uid\",\"state\",\"year_month\",\"payer_slug\",\"billing_class\",\"code_type\",\"code\",\n",
    "          \"pg_uid\",\"pos_set_id\",\"negotiated_type\",\"negotiation_arrangement\",\n",
    "          \"negotiated_rate\",\"expiration_date\",\"provider_group_id_raw\",\"reporting_entity_name\"\n",
    "      )\n",
    "      .unique()\n",
    ")\n",
    "\n",
    "print(\"fact_new rows:\", fact_new.height)\n",
    "fact_new.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e92746",
   "metadata": {},
   "source": [
    "cell 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93eb37c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 8: robust upsert that normalizes schema before insert ---\n",
    "import os, duckdb\n",
    "\n",
    "CANON_COLS_SQL = \"\"\"\n",
    "  CAST(fact_uid                AS VARCHAR) AS fact_uid,\n",
    "  CAST(state                   AS VARCHAR) AS state,\n",
    "  CAST(year_month              AS VARCHAR) AS year_month,\n",
    "  CAST(payer_slug              AS VARCHAR) AS payer_slug,\n",
    "  CAST(billing_class           AS VARCHAR) AS billing_class,\n",
    "  CAST(code_type               AS VARCHAR) AS code_type,\n",
    "  CAST(code                    AS VARCHAR) AS code,\n",
    "  CAST(pg_uid                  AS VARCHAR) AS pg_uid,\n",
    "  CAST(pos_set_id              AS VARCHAR) AS pos_set_id,\n",
    "  CAST(negotiated_type         AS VARCHAR) AS negotiated_type,\n",
    "  CAST(negotiation_arrangement AS VARCHAR) AS negotiation_arrangement,\n",
    "  CAST(negotiated_rate         AS DOUBLE)  AS negotiated_rate,\n",
    "  CAST(expiration_date         AS VARCHAR) AS expiration_date,\n",
    "  CAST(provider_group_id_raw   AS VARCHAR) AS provider_group_id_raw,\n",
    "  CAST(reporting_entity_name   AS VARCHAR) AS reporting_entity_name\n",
    "\"\"\"\n",
    "\n",
    "def upsert_fact_single(fact_batch: pl.DataFrame):\n",
    "    # write the incoming batch to a temp parquet\n",
    "    tmp_new = f\"{GOLD_FACT_FILE}.stage.parquet\"\n",
    "    tmp_out = f\"{GOLD_FACT_FILE}.new.parquet\"\n",
    "    fact_batch.write_parquet(tmp_new)\n",
    "\n",
    "    p_all  = GOLD_FACT_FILE.replace(\"'\", \"''\")\n",
    "    p_new  = tmp_new.replace(\"'\", \"''\")\n",
    "    p_out  = tmp_out.replace(\"'\", \"''\")\n",
    "\n",
    "    con = duckdb.connect()  # ephemeral in-memory db/session\n",
    "\n",
    "    if not os.path.exists(GOLD_FACT_FILE):\n",
    "        # First write: just normalize schema and save\n",
    "        con.execute(f\"CREATE OR REPLACE TABLE _stage AS SELECT {CANON_COLS_SQL} FROM read_parquet('{p_new}');\")\n",
    "        con.execute(f\"COPY (SELECT * FROM _stage) TO '{p_all}' (FORMAT PARQUET, COMPRESSION ZSTD);\")\n",
    "        con.close()\n",
    "        os.remove(tmp_new)\n",
    "        print(f\"Created {GOLD_FACT_FILE} with {fact_batch.height} rows.\")\n",
    "        return\n",
    "\n",
    "    # Normalize both existing gold and new batch to the canonical schema\n",
    "    con.execute(f\"CREATE OR REPLACE TABLE _all   AS SELECT {CANON_COLS_SQL} FROM read_parquet('{p_all}');\")\n",
    "    con.execute(f\"CREATE OR REPLACE TABLE _stage AS SELECT {CANON_COLS_SQL} FROM read_parquet('{p_new}');\")\n",
    "\n",
    "    # Insert only new fact_uids\n",
    "    con.execute(\"\"\"\n",
    "      INSERT INTO _all\n",
    "      SELECT s.*\n",
    "      FROM _stage s\n",
    "      LEFT JOIN _all a ON a.fact_uid = s.fact_uid\n",
    "      WHERE a.fact_uid IS NULL;\n",
    "    \"\"\")\n",
    "\n",
    "    # Write back to parquet with consistent schema\n",
    "    con.execute(f\"COPY (SELECT * FROM _all) TO '{p_out}' (FORMAT PARQUET, COMPRESSION ZSTD);\")\n",
    "    con.close()\n",
    "    os.replace(tmp_out, GOLD_FACT_FILE)\n",
    "    os.remove(tmp_new)\n",
    "    print(f\"Upsert complete into {GOLD_FACT_FILE}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec88fc23",
   "metadata": {},
   "source": [
    "Cell 9 send fact rate to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60b1ab69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec54293aaec482c97ec2cf23f91ffcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created c:\\Users\\ChristopherCato\\OneDrive - clarity-dx.com\\code\\bph\\workcomp-rates-etl\\data\\gold\\fact_rate.parquet with 2967105 rows.\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 - Actually save the fact table (FIXED VERSION)\n",
    "import os, duckdb\n",
    "\n",
    "CANON_COLS_SQL = \"\"\"\n",
    "  CAST(fact_uid                AS VARCHAR) AS fact_uid,\n",
    "  CAST(state                   AS VARCHAR) AS state,\n",
    "  CAST(year_month              AS VARCHAR) AS year_month,\n",
    "  CAST(payer_slug              AS VARCHAR) AS payer_slug,\n",
    "  CAST(billing_class           AS VARCHAR) AS billing_class,\n",
    "  CAST(code_type               AS VARCHAR) AS code_type,\n",
    "  CAST(code                    AS VARCHAR) AS code,\n",
    "  CAST(pg_uid                  AS VARCHAR) AS pg_uid,\n",
    "  CAST(pos_set_id              AS VARCHAR) AS pos_set_id,\n",
    "  CAST(negotiated_type         AS VARCHAR) AS negotiated_type,\n",
    "  CAST(negotiation_arrangement AS VARCHAR) AS negotiation_arrangement,\n",
    "  CAST(negotiated_rate         AS DOUBLE)  AS negotiated_rate,\n",
    "  CAST(expiration_date         AS VARCHAR) AS expiration_date,\n",
    "  CAST(provider_group_id_raw   AS VARCHAR) AS provider_group_id_raw,\n",
    "  CAST(reporting_entity_name   AS VARCHAR) AS reporting_entity_name\n",
    "\"\"\"\n",
    "\n",
    "def upsert_fact_single_fixed(fact_batch: pl.DataFrame):\n",
    "    # write the incoming batch to a temp parquet\n",
    "    tmp_new = f\"{GOLD_FACT_FILE}.stage.parquet\"\n",
    "    tmp_out = f\"{GOLD_FACT_FILE}.new.parquet\"\n",
    "    fact_batch.write_parquet(tmp_new)\n",
    "\n",
    "    # Convert Path objects to strings for SQL\n",
    "    p_all  = str(GOLD_FACT_FILE).replace(\"'\", \"''\")\n",
    "    p_new  = tmp_new.replace(\"'\", \"''\")\n",
    "    p_out  = tmp_out.replace(\"'\", \"''\")\n",
    "\n",
    "    con = duckdb.connect()  # ephemeral in-memory db/session\n",
    "\n",
    "    if not os.path.exists(GOLD_FACT_FILE):\n",
    "        # First write: just normalize schema and save\n",
    "        con.execute(f\"CREATE OR REPLACE TABLE _stage AS SELECT {CANON_COLS_SQL} FROM read_parquet('{p_new}');\")\n",
    "        con.execute(f\"COPY (SELECT * FROM _stage) TO '{p_all}' (FORMAT PARQUET, COMPRESSION ZSTD);\")\n",
    "        con.close()\n",
    "        os.remove(tmp_new)\n",
    "        print(f\"Created {GOLD_FACT_FILE} with {fact_batch.height} rows.\")\n",
    "        return\n",
    "\n",
    "    # Normalize both existing gold and new batch to the canonical schema\n",
    "    con.execute(f\"CREATE OR REPLACE TABLE _all   AS SELECT {CANON_COLS_SQL} FROM read_parquet('{p_all}');\")\n",
    "    con.execute(f\"CREATE OR REPLACE TABLE _stage AS SELECT {CANON_COLS_SQL} FROM read_parquet('{p_new}');\")\n",
    "\n",
    "    # Insert only new fact_uids\n",
    "    con.execute(\"\"\"\n",
    "      INSERT INTO _all\n",
    "      SELECT s.*\n",
    "      FROM _stage s\n",
    "      LEFT JOIN _all a ON a.fact_uid = s.fact_uid\n",
    "      WHERE a.fact_uid IS NULL;\n",
    "    \"\"\")\n",
    "\n",
    "    # Write back to parquet with consistent schema\n",
    "    con.execute(f\"COPY (SELECT * FROM _all) TO '{p_out}' (FORMAT PARQUET, COMPRESSION ZSTD);\")\n",
    "    con.close()\n",
    "    os.replace(tmp_out, GOLD_FACT_FILE)\n",
    "    os.remove(tmp_new)\n",
    "    print(f\"Upsert complete into {GOLD_FACT_FILE}.\")\n",
    "\n",
    "# Actually save the fact table\n",
    "upsert_fact_single_fixed(fact_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92be989d",
   "metadata": {},
   "source": [
    "Cell 9 - Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bd87917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row counts:\n",
      "  dim_code         : 3696\n",
      "  dim_payer        : 1\n",
      "  dim_provider_grp : 637\n",
      "  dim_pos_set      : 4\n",
      "  xref_pg_npi      : 16999\n",
      "  xref_pg_tin      : 9486\n",
      "  fact_rate (gold) : 2967105\n"
     ]
    }
   ],
   "source": [
    "def count_parquet_rows(path: Path) -> int:\n",
    "    if not path.exists():\n",
    "        return 0\n",
    "    con = duckdb.connect()\n",
    "    n = con.execute(f\"SELECT COUNT(*) FROM read_parquet('{str(path)}')\").fetchone()[0]\n",
    "    con.close()\n",
    "    return int(n)\n",
    "\n",
    "print(\"Row counts:\")\n",
    "print(\"  dim_code         :\", count_parquet_rows(DIM_CODE_FILE))\n",
    "print(\"  dim_payer        :\", count_parquet_rows(DIM_PAYER_FILE))\n",
    "print(\"  dim_provider_grp :\", count_parquet_rows(DIM_PG_FILE))\n",
    "print(\"  dim_pos_set      :\", count_parquet_rows(DIM_POS_FILE))\n",
    "print(\"  xref_pg_npi      :\", count_parquet_rows(XREF_PG_NPI))\n",
    "print(\"  xref_pg_tin      :\", count_parquet_rows(XREF_PG_TIN))\n",
    "print(\"  fact_rate (gold) :\", count_parquet_rows(GOLD_FACT_FILE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d994c7",
   "metadata": {},
   "source": [
    "checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f966a47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BASIC COUNTS ===\n",
      "Fact table: 2,967,105 rows\n",
      "Dim codes: 3,696 rows\n",
      "Dim payers: 1 rows\n",
      "Dim provider groups: 637 rows\n",
      "Dim POS sets: 4 rows\n",
      "Xref NPI: 16,999 rows\n",
      "Xref TIN: 9,486 rows\n",
      "\n",
      "=== FOREIGN KEY INTEGRITY ===\n",
      "Fact -> Dim Code mapping:\n",
      "  Missing codes: 0\n",
      "Fact -> Dim Payer mapping:\n",
      "  Missing payers: 0\n",
      "Fact -> Dim Provider Group mapping:\n",
      "  Missing provider groups: 0\n",
      "Fact -> Dim POS Set mapping:\n",
      "  Missing POS sets: 0\n",
      "\n",
      "=== CROSS-REFERENCE INTEGRITY ===\n",
      "Provider groups with NPI mappings: 13,912\n",
      "Provider groups with TIN mappings: 7,661\n",
      "\n",
      "=== SAMPLE DATA OVERLAP ===\n",
      "Sample joined data:\n",
      "shape: (5, 12)\n",
      "┌────────────┬───────┬────────────┬────────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
      "│ fact_uid   ┆ state ┆ payer_slug ┆ reporting_ ┆ … ┆ provider_ ┆ pos_set_i ┆ pos_membe ┆ negotiate │\n",
      "│ ---        ┆ ---   ┆ ---        ┆ entity_nam ┆   ┆ group_id_ ┆ d         ┆ rs        ┆ d_rate    │\n",
      "│ str        ┆ str   ┆ str        ┆ e          ┆   ┆ raw       ┆ ---       ┆ ---       ┆ ---       │\n",
      "│            ┆       ┆            ┆ ---        ┆   ┆ ---       ┆ str       ┆ list[str] ┆ f64       │\n",
      "│            ┆       ┆            ┆ str        ┆   ┆ str       ┆           ┆           ┆           │\n",
      "╞════════════╪═══════╪════════════╪════════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
      "│ d023eebdae ┆ GA    ┆ unitedheal ┆ UnitedHeal ┆ … ┆ 584       ┆ 17b00c58b ┆ [\"02\",    ┆ 608.27    │\n",
      "│ 2fec46f917 ┆       ┆ thcare-of- ┆ thcare of  ┆   ┆           ┆ 3dcdb9c20 ┆ \"05\", …   ┆           │\n",
      "│ 9ab625c678 ┆       ┆ georgia-in ┆ Georgia    ┆   ┆           ┆ cb2a70b52 ┆ \"61\"]     ┆           │\n",
      "│ …          ┆       ┆ …          ┆ In…        ┆   ┆           ┆ a4c…      ┆           ┆           │\n",
      "│ 5f8783b3c2 ┆ GA    ┆ unitedheal ┆ UnitedHeal ┆ … ┆ 282       ┆ 17b00c58b ┆ [\"02\",    ┆ 1051.15   │\n",
      "│ eadafc80af ┆       ┆ thcare-of- ┆ thcare of  ┆   ┆           ┆ 3dcdb9c20 ┆ \"05\", …   ┆           │\n",
      "│ 9f47a3bcec ┆       ┆ georgia-in ┆ Georgia    ┆   ┆           ┆ cb2a70b52 ┆ \"61\"]     ┆           │\n",
      "│ …          ┆       ┆ …          ┆ In…        ┆   ┆           ┆ a4c…      ┆           ┆           │\n",
      "│ da6893a29f ┆ GA    ┆ unitedheal ┆ UnitedHeal ┆ … ┆ 220       ┆ 3b3743c43 ┆ [\"l\",     ┆ 0.0       │\n",
      "│ 62eb80b35f ┆       ┆ thcare-of- ┆ thcare of  ┆   ┆           ┆ fbdc8f1b8 ┆ \"n\", \"u\"] ┆           │\n",
      "│ c28450365a ┆       ┆ georgia-in ┆ Georgia    ┆   ┆           ┆ eedc16b26 ┆           ┆           │\n",
      "│ …          ┆       ┆ …          ┆ In…        ┆   ┆           ┆ 4f3…      ┆           ┆           │\n",
      "│ 4df7a8e828 ┆ GA    ┆ unitedheal ┆ UnitedHeal ┆ … ┆ 359       ┆ 17b00c58b ┆ [\"02\",    ┆ 207.74    │\n",
      "│ 5bad9e4014 ┆       ┆ thcare-of- ┆ thcare of  ┆   ┆           ┆ 3dcdb9c20 ┆ \"05\", …   ┆           │\n",
      "│ a0a831fc61 ┆       ┆ georgia-in ┆ Georgia    ┆   ┆           ┆ cb2a70b52 ┆ \"61\"]     ┆           │\n",
      "│ …          ┆       ┆ …          ┆ In…        ┆   ┆           ┆ a4c…      ┆           ┆           │\n",
      "│ 21e52eb782 ┆ GA    ┆ unitedheal ┆ UnitedHeal ┆ … ┆ 886       ┆ 17b00c58b ┆ [\"02\",    ┆ 53.67     │\n",
      "│ 054104eb23 ┆       ┆ thcare-of- ┆ thcare of  ┆   ┆           ┆ 3dcdb9c20 ┆ \"05\", …   ┆           │\n",
      "│ 00e979b80e ┆       ┆ georgia-in ┆ Georgia    ┆   ┆           ┆ cb2a70b52 ┆ \"61\"]     ┆           │\n",
      "│ …          ┆       ┆ …          ┆ In…        ┆   ┆           ┆ a4c…      ┆           ┆           │\n",
      "└────────────┴───────┴────────────┴────────────┴───┴───────────┴───────────┴───────────┴───────────┘\n",
      "\n",
      "=== UNIQUENESS CHECKS ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ChristopherCato\\AppData\\Local\\Temp\\ipykernel_4280\\2586700953.py:114: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "  duplicate_facts = fact.group_by(\"fact_uid\").agg(pl.count().alias(\"count\")).filter(pl.col(\"count\") > 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate fact_uids: 0\n",
      "Duplicate code keys: 0\n",
      "Duplicate payer keys: 0\n",
      "Duplicate provider group keys: 0\n",
      "\n",
      "=== DATA QUALITY SUMMARY ===\n",
      "✅ All checks passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ChristopherCato\\AppData\\Local\\Temp\\ipykernel_4280\\2586700953.py:118: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "  duplicate_codes = dim_code.group_by([\"code_type\", \"code\"]).agg(pl.count().alias(\"count\")).filter(pl.col(\"count\") > 1)\n",
      "C:\\Users\\ChristopherCato\\AppData\\Local\\Temp\\ipykernel_4280\\2586700953.py:121: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "  duplicate_payers = dim_payer.group_by(\"payer_slug\").agg(pl.count().alias(\"count\")).filter(pl.col(\"count\") > 1)\n",
      "C:\\Users\\ChristopherCato\\AppData\\Local\\Temp\\ipykernel_4280\\2586700953.py:124: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "  duplicate_pgs = dim_pg.group_by(\"pg_uid\").agg(pl.count().alias(\"count\")).filter(pl.col(\"count\") > 1)\n"
     ]
    }
   ],
   "source": [
    "# Data Integrity & Relationship Verification\n",
    "import polars as pl\n",
    "import duckdb\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Helper to safely load a parquet file, returning None if missing\n",
    "def safe_read_parquet(path):\n",
    "    try:\n",
    "        return pl.read_parquet(path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ File not found: {path}\")\n",
    "        return None\n",
    "\n",
    "# All file paths are relative to the notebook location (ETL/), so back out one level\n",
    "base_dir = os.path.join(\"..\", \"data\")\n",
    "fact_path = os.path.join(base_dir, \"gold\", \"fact_rate.parquet\")\n",
    "dim_code_path = os.path.join(base_dir, \"dims\", \"dim_code.parquet\")\n",
    "dim_payer_path = os.path.join(base_dir, \"dims\", \"dim_payer.parquet\")\n",
    "dim_pg_path = os.path.join(base_dir, \"dims\", \"dim_provider_group.parquet\")\n",
    "dim_pos_path = os.path.join(base_dir, \"dims\", \"dim_pos_set.parquet\")\n",
    "xref_npi_path = os.path.join(base_dir, \"xrefs\", \"xref_pg_member_npi.parquet\")\n",
    "xref_tin_path = os.path.join(base_dir, \"xrefs\", \"xref_pg_member_tin.parquet\")\n",
    "\n",
    "# Load all the output tables, handling missing files gracefully\n",
    "fact = safe_read_parquet(fact_path)\n",
    "dim_code = safe_read_parquet(dim_code_path)\n",
    "dim_payer = safe_read_parquet(dim_payer_path)\n",
    "dim_pg = safe_read_parquet(dim_pg_path)\n",
    "dim_pos = safe_read_parquet(dim_pos_path)\n",
    "xref_npi = safe_read_parquet(xref_npi_path)\n",
    "xref_tin = safe_read_parquet(xref_tin_path)\n",
    "\n",
    "# If any required table is missing, skip the rest of the checks\n",
    "required_tables = {\n",
    "    \"fact\": fact,\n",
    "    \"dim_code\": dim_code,\n",
    "    \"dim_payer\": dim_payer,\n",
    "    \"dim_pg\": dim_pg,\n",
    "    \"dim_pos\": dim_pos,\n",
    "    \"xref_npi\": xref_npi,\n",
    "    \"xref_tin\": xref_tin,\n",
    "}\n",
    "missing = [k for k, v in required_tables.items() if v is None]\n",
    "if missing:\n",
    "    print(f\"\\n❌ Skipping integrity checks. Missing tables: {', '.join(missing)}\")\n",
    "else:\n",
    "    print(\"=== BASIC COUNTS ===\")\n",
    "    print(f\"Fact table: {fact.height:,} rows\")\n",
    "    print(f\"Dim codes: {dim_code.height:,} rows\")\n",
    "    print(f\"Dim payers: {dim_payer.height:,} rows\")\n",
    "    print(f\"Dim provider groups: {dim_pg.height:,} rows\")\n",
    "    print(f\"Dim POS sets: {dim_pos.height:,} rows\")\n",
    "    print(f\"Xref NPI: {xref_npi.height:,} rows\")\n",
    "    print(f\"Xref TIN: {xref_tin.height:,} rows\")\n",
    "\n",
    "    print(\"\\n=== FOREIGN KEY INTEGRITY ===\")\n",
    "\n",
    "    # Check if all fact table foreign keys exist in dimensions\n",
    "    print(\"Fact -> Dim Code mapping:\")\n",
    "    fact_codes = fact.select([\"code_type\", \"code\"]).unique()\n",
    "    missing_codes = fact_codes.join(dim_code, on=[\"code_type\", \"code\"], how=\"anti\")\n",
    "    print(f\"  Missing codes: {missing_codes.height}\")\n",
    "\n",
    "    print(\"Fact -> Dim Payer mapping:\")\n",
    "    fact_payers = fact.select(\"payer_slug\").unique()\n",
    "    missing_payers = fact_payers.join(dim_payer, on=\"payer_slug\", how=\"anti\")\n",
    "    print(f\"  Missing payers: {missing_payers.height}\")\n",
    "\n",
    "    print(\"Fact -> Dim Provider Group mapping:\")\n",
    "    fact_pgs = fact.select(\"pg_uid\").unique()\n",
    "    missing_pgs = fact_pgs.join(dim_pg, on=\"pg_uid\", how=\"anti\")\n",
    "    print(f\"  Missing provider groups: {missing_pgs.height}\")\n",
    "\n",
    "    print(\"Fact -> Dim POS Set mapping:\")\n",
    "    fact_pos = fact.select(\"pos_set_id\").unique()\n",
    "    missing_pos = fact_pos.join(dim_pos, on=\"pos_set_id\", how=\"anti\")\n",
    "    print(f\"  Missing POS sets: {missing_pos.height}\")\n",
    "\n",
    "    print(\"\\n=== CROSS-REFERENCE INTEGRITY ===\")\n",
    "\n",
    "    # Check if all provider groups in fact table have NPI mappings\n",
    "    fact_pg_has_npi = fact.select(\"pg_uid\").unique().join(xref_npi, on=\"pg_uid\", how=\"inner\")\n",
    "    print(f\"Provider groups with NPI mappings: {fact_pg_has_npi.height:,}\")\n",
    "\n",
    "    # Check if all provider groups in fact table have TIN mappings  \n",
    "    fact_pg_has_tin = fact.select(\"pg_uid\").unique().join(xref_tin, on=\"pg_uid\", how=\"inner\")\n",
    "    print(f\"Provider groups with TIN mappings: {fact_pg_has_tin.height:,}\")\n",
    "\n",
    "    print(\"\\n=== SAMPLE DATA OVERLAP ===\")\n",
    "\n",
    "    # Show a sample of joined data to verify relationships\n",
    "    sample_joined = (\n",
    "        fact.head(5)\n",
    "        .join(dim_code, on=[\"code_type\", \"code\"], how=\"left\")\n",
    "        .join(dim_payer, on=\"payer_slug\", how=\"left\")\n",
    "        .join(dim_pg, on=\"pg_uid\", how=\"left\")\n",
    "        .join(dim_pos, on=\"pos_set_id\", how=\"left\")\n",
    "        .select([\n",
    "            \"fact_uid\", \"state\", \"payer_slug\", \"reporting_entity_name\",\n",
    "            \"code_type\", \"code\", \"code_description\", \n",
    "            \"pg_uid\", \"provider_group_id_raw\",\n",
    "            \"pos_set_id\", \"pos_members\",\n",
    "            \"negotiated_rate\"\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    print(\"Sample joined data:\")\n",
    "    print(sample_joined)\n",
    "\n",
    "    print(\"\\n=== UNIQUENESS CHECKS ===\")\n",
    "\n",
    "    # Check for duplicate fact_uids (should be 0)\n",
    "    duplicate_facts = fact.group_by(\"fact_uid\").agg(pl.count().alias(\"count\")).filter(pl.col(\"count\") > 1)\n",
    "    print(f\"Duplicate fact_uids: {duplicate_facts.height}\")\n",
    "\n",
    "    # Check for duplicate dimension keys\n",
    "    duplicate_codes = dim_code.group_by([\"code_type\", \"code\"]).agg(pl.count().alias(\"count\")).filter(pl.col(\"count\") > 1)\n",
    "    print(f\"Duplicate code keys: {duplicate_codes.height}\")\n",
    "\n",
    "    duplicate_payers = dim_payer.group_by(\"payer_slug\").agg(pl.count().alias(\"count\")).filter(pl.col(\"count\") > 1)\n",
    "    print(f\"Duplicate payer keys: {duplicate_payers.height}\")\n",
    "\n",
    "    duplicate_pgs = dim_pg.group_by(\"pg_uid\").agg(pl.count().alias(\"count\")).filter(pl.col(\"count\") > 1)\n",
    "    print(f\"Duplicate provider group keys: {duplicate_pgs.height}\")\n",
    "\n",
    "    print(\"\\n=== DATA QUALITY SUMMARY ===\")\n",
    "    print(\"✅ All checks passed!\" if all([\n",
    "        missing_codes.height == 0,\n",
    "        missing_payers.height == 0, \n",
    "        missing_pgs.height == 0,\n",
    "        missing_pos.height == 0,\n",
    "        duplicate_facts.height == 0,\n",
    "        duplicate_codes.height == 0,\n",
    "        duplicate_payers.height == 0,\n",
    "        duplicate_pgs.height == 0\n",
    "    ]) else \"❌ Some integrity issues found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d570994d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUGGING CROSS-REFERENCE ISSUE ===\n",
      "Unique pg_uids in fact table: 637\n",
      "Unique pg_uids in NPI xref: 952\n",
      "Unique pg_uids in TIN xref: 952\n",
      "Fact pg_uids that have NPI mappings: 637\n",
      "Fact pg_uids that have TIN mappings: 637\n",
      "\n",
      "=== SAMPLE PG_UIDS ===\n",
      "Sample fact pg_uids:\n",
      "shape: (3, 1)\n",
      "┌─────────────────────────────────┐\n",
      "│ pg_uid                          │\n",
      "│ ---                             │\n",
      "│ str                             │\n",
      "╞═════════════════════════════════╡\n",
      "│ 0c9046c67c2d317c91e95bdc68d6db… │\n",
      "│ 833152a0034c3b8b3455a111829fa9… │\n",
      "│ 440382b2ba13cc0f8787058d848e86… │\n",
      "└─────────────────────────────────┘\n",
      "\n",
      "Sample NPI xref pg_uids:\n",
      "shape: (3, 1)\n",
      "┌─────────────────────────────────┐\n",
      "│ pg_uid                          │\n",
      "│ ---                             │\n",
      "│ str                             │\n",
      "╞═════════════════════════════════╡\n",
      "│ d04a4401f5a42f6d03968655f76530… │\n",
      "│ b45822a29cb9073d5906c10bab59fb… │\n",
      "│ a910491ce9955a0083ee09c0b9c36d… │\n",
      "└─────────────────────────────────┘\n",
      "\n",
      "Sample TIN xref pg_uids:\n",
      "shape: (3, 1)\n",
      "┌─────────────────────────────────┐\n",
      "│ pg_uid                          │\n",
      "│ ---                             │\n",
      "│ str                             │\n",
      "╞═════════════════════════════════╡\n",
      "│ 645a0826bf031f58221e25db124bf2… │\n",
      "│ 7a7f88ce5028e79f933e433f3f4744… │\n",
      "│ 087f88877e800afa9de5424c3f4392… │\n",
      "└─────────────────────────────────┘\n",
      "\n",
      "NPI xref pg_uids NOT in fact table: 315\n",
      "TIN xref pg_uids NOT in fact table: 315\n",
      "\n",
      "Sample NPI xref pg_uids not in fact:\n",
      "shape: (3, 1)\n",
      "┌─────────────────────────────────┐\n",
      "│ pg_uid                          │\n",
      "│ ---                             │\n",
      "│ str                             │\n",
      "╞═════════════════════════════════╡\n",
      "│ 2a1fa4b166602fa73b9747e3559b1e… │\n",
      "│ feb6f751bb8221bdebb019fa89562d… │\n",
      "│ 4cb8b985821c5d224d9660d2cfb01e… │\n",
      "└─────────────────────────────────┘\n",
      "\n",
      "Sample TIN xref pg_uids not in fact:\n",
      "shape: (3, 1)\n",
      "┌─────────────────────────────────┐\n",
      "│ pg_uid                          │\n",
      "│ ---                             │\n",
      "│ str                             │\n",
      "╞═════════════════════════════════╡\n",
      "│ 4ef99f4c6f2665f14920f0817f0a58… │\n",
      "│ bf5cc891395b769382a1bd89155a90… │\n",
      "│ c45cb66b0c8573f0e2932385b485ef… │\n",
      "└─────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Debug the cross-reference mapping issue\n",
    "import polars as pl\n",
    "\n",
    "# Load the tables (back out a level, then go under data, not core)\n",
    "fact = pl.read_parquet(\"../data/gold/fact_rate.parquet\")\n",
    "xref_npi = pl.read_parquet(\"../data/xrefs/xref_pg_member_npi.parquet\")\n",
    "xref_tin = pl.read_parquet(\"../data/xrefs/xref_pg_member_tin.parquet\")\n",
    "\n",
    "print(\"=== DEBUGGING CROSS-REFERENCE ISSUE ===\")\n",
    "\n",
    "# Check what pg_uids exist in fact vs xref tables\n",
    "fact_pg_uids = fact.select(\"pg_uid\").unique()\n",
    "xref_npi_pg_uids = xref_npi.select(\"pg_uid\").unique()\n",
    "xref_tin_pg_uids = xref_tin.select(\"pg_uid\").unique()\n",
    "\n",
    "print(f\"Unique pg_uids in fact table: {fact_pg_uids.height:,}\")\n",
    "print(f\"Unique pg_uids in NPI xref: {xref_npi_pg_uids.height:,}\")\n",
    "print(f\"Unique pg_uids in TIN xref: {xref_tin_pg_uids.height:,}\")\n",
    "\n",
    "# Check for overlap\n",
    "fact_npi_overlap = fact_pg_uids.join(xref_npi_pg_uids, on=\"pg_uid\", how=\"inner\")\n",
    "fact_tin_overlap = fact_pg_uids.join(xref_tin_pg_uids, on=\"pg_uid\", how=\"inner\")\n",
    "\n",
    "print(f\"Fact pg_uids that have NPI mappings: {fact_npi_overlap.height:,}\")\n",
    "print(f\"Fact pg_uids that have TIN mappings: {fact_tin_overlap.height:,}\")\n",
    "\n",
    "# Show some sample pg_uids from each table to see if they look different\n",
    "print(\"\\n=== SAMPLE PG_UIDS ===\")\n",
    "print(\"Sample fact pg_uids:\")\n",
    "print(fact_pg_uids.head(3))\n",
    "print(\"\\nSample NPI xref pg_uids:\")\n",
    "print(xref_npi_pg_uids.head(3))\n",
    "print(\"\\nSample TIN xref pg_uids:\")\n",
    "print(xref_tin_pg_uids.head(3))\n",
    "\n",
    "# Check if there are any pg_uids that exist in xref but not in fact\n",
    "npi_not_in_fact = xref_npi_pg_uids.join(fact_pg_uids, on=\"pg_uid\", how=\"anti\")\n",
    "tin_not_in_fact = xref_tin_pg_uids.join(fact_pg_uids, on=\"pg_uid\", how=\"anti\")\n",
    "\n",
    "print(f\"\\nNPI xref pg_uids NOT in fact table: {npi_not_in_fact.height:,}\")\n",
    "print(f\"TIN xref pg_uids NOT in fact table: {tin_not_in_fact.height:,}\")\n",
    "\n",
    "# If there's a mismatch, show some examples\n",
    "if npi_not_in_fact.height > 0:\n",
    "    print(\"\\nSample NPI xref pg_uids not in fact:\")\n",
    "    print(npi_not_in_fact.head(3))\n",
    "\n",
    "if tin_not_in_fact.height > 0:\n",
    "    print(\"\\nSample TIN xref pg_uids not in fact:\")\n",
    "    print(tin_not_in_fact.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dc0827",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING PG_UID FIX ===\n",
      "Unique pg_uids in fact table: 637\n",
      "Unique pg_uids in NPI xref: 952\n",
      "Unique pg_uids in TIN xref: 952\n",
      "\n",
      "Fact pg_uids that have NPI mappings: 637\n",
      "Fact pg_uids that have TIN mappings: 637\n",
      "\n",
      "=== SAMPLE MATCHING PG_UIDS ===\n",
      "Sample matching pg_uids:\n",
      "shape: (3, 1)\n",
      "┌─────────────────────────────────┐\n",
      "│ pg_uid                          │\n",
      "│ ---                             │\n",
      "│ str                             │\n",
      "╞═════════════════════════════════╡\n",
      "│ 2fb9b1fcfceaec741dd7ea83bde1f6… │\n",
      "│ f58ad428df5da16b81c443a4e72868… │\n",
      "│ fdb18ec69c97eb87293fae8b7fa4a8… │\n",
      "└─────────────────────────────────┘\n",
      "\n",
      "Data for pg_uid: 2fb9b1fcfceaec741dd7ea83bde1f60c\n",
      "Fact table sample:\n",
      "shape: (1, 3)\n",
      "┌─────────────────────────────────┬───────────────────────┬─────────────────────────────────┐\n",
      "│ pg_uid                          ┆ provider_group_id_raw ┆ payer_slug                      │\n",
      "│ ---                             ┆ ---                   ┆ ---                             │\n",
      "│ str                             ┆ str                   ┆ str                             │\n",
      "╞═════════════════════════════════╪═══════════════════════╪═════════════════════════════════╡\n",
      "│ 2fb9b1fcfceaec741dd7ea83bde1f6… ┆ 876                   ┆ unitedhealthcare-of-georgia-in… │\n",
      "└─────────────────────────────────┴───────────────────────┴─────────────────────────────────┘\n",
      "NPI xref sample:\n",
      "shape: (1, 2)\n",
      "┌─────────────────────────────────┬────────────┐\n",
      "│ pg_uid                          ┆ npi        │\n",
      "│ ---                             ┆ ---        │\n",
      "│ str                             ┆ str        │\n",
      "╞═════════════════════════════════╪════════════╡\n",
      "│ 2fb9b1fcfceaec741dd7ea83bde1f6… ┆ 1780643411 │\n",
      "└─────────────────────────────────┴────────────┘\n",
      "TIN xref sample:\n",
      "shape: (1, 3)\n",
      "┌─────────────────────────────────┬──────────┬───────────┐\n",
      "│ pg_uid                          ┆ tin_type ┆ tin_value │\n",
      "│ ---                             ┆ ---      ┆ ---       │\n",
      "│ str                             ┆ str      ┆ str       │\n",
      "╞═════════════════════════════════╪══════════╪═══════════╡\n",
      "│ 2fb9b1fcfceaec741dd7ea83bde1f6… ┆ ein      ┆ 352228583 │\n",
      "└─────────────────────────────────┴──────────┴───────────┘\n",
      "\n",
      "=== SUCCESS CHECK ===\n",
      "✅ SUCCESS! pg_uids now match between fact and cross-reference tables!\n",
      "✅ The ETL fix worked!\n"
     ]
    }
   ],
   "source": [
    "# Test the pg_uid fix\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "print(\"=== TESTING PG_UID FIX ===\")\n",
    "\n",
    "# Check what pg_uids exist in fact vs xref tables\n",
    "fact_pg_uids = fact.select(\"pg_uid\").unique()\n",
    "xref_npi_pg_uids = xref_npi.select(\"pg_uid\").unique()\n",
    "xref_tin_pg_uids = xref_tin.select(\"pg_uid\").unique()\n",
    "\n",
    "print(f\"Unique pg_uids in fact table: {fact_pg_uids.height:,}\")\n",
    "print(f\"Unique pg_uids in NPI xref: {xref_npi_pg_uids.height:,}\")\n",
    "print(f\"Unique pg_uids in TIN xref: {xref_tin_pg_uids.height:,}\")\n",
    "\n",
    "# Check for overlap (this should now be > 0!)\n",
    "fact_npi_overlap = fact_pg_uids.join(xref_npi_pg_uids, on=\"pg_uid\", how=\"inner\")\n",
    "fact_tin_overlap = fact_pg_uids.join(xref_tin_pg_uids, on=\"pg_uid\", how=\"inner\")\n",
    "\n",
    "print(f\"\\nFact pg_uids that have NPI mappings: {fact_npi_overlap.height:,}\")\n",
    "print(f\"Fact pg_uids that have TIN mappings: {fact_tin_overlap.height:,}\")\n",
    "\n",
    "# Show some sample matching pg_uids\n",
    "if fact_npi_overlap.height > 0:\n",
    "    print(\"\\n=== SAMPLE MATCHING PG_UIDS ===\")\n",
    "    sample_matches = fact_npi_overlap.head(3)\n",
    "    print(\"Sample matching pg_uids:\")\n",
    "    print(sample_matches)\n",
    "    \n",
    "    # Show the actual data for one matching pg_uid\n",
    "    sample_pg_uid = sample_matches.item(0, 0)\n",
    "    print(f\"\\nData for pg_uid: {sample_pg_uid}\")\n",
    "    \n",
    "    fact_sample = fact.filter(pl.col(\"pg_uid\") == sample_pg_uid).head(1)\n",
    "    npi_sample = xref_npi.filter(pl.col(\"pg_uid\") == sample_pg_uid).head(1)\n",
    "    tin_sample = xref_tin.filter(pl.col(\"pg_uid\") == sample_pg_uid).head(1)\n",
    "    \n",
    "    print(\"Fact table sample:\")\n",
    "    print(fact_sample.select([\"pg_uid\", \"provider_group_id_raw\", \"payer_slug\"]))\n",
    "    print(\"NPI xref sample:\")\n",
    "    print(npi_sample)\n",
    "    print(\"TIN xref sample:\")\n",
    "    print(tin_sample)\n",
    "\n",
    "print(\"\\n=== SUCCESS CHECK ===\")\n",
    "if fact_npi_overlap.height > 0 and fact_tin_overlap.height > 0:\n",
    "    print(\"✅ SUCCESS! pg_uids now match between fact and cross-reference tables!\")\n",
    "    print(\"✅ The ETL fix worked!\")\n",
    "else:\n",
    "    print(\"❌ Still no matches. Need to investigate further.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
